# ğŸ¤– Model Explanation (ëª¨ë¸ ì„¤ëª…)

## ëª©ì°¨
1. [ëª¨ë¸ ê°œìš”](#ëª¨ë¸-ê°œìš”)
2. [Random Forest ì•Œê³ ë¦¬ì¦˜](#random-forest-ì•Œê³ ë¦¬ì¦˜)
3. [í•˜ì´í¼íŒŒë¼ë¯¸í„°](#í•˜ì´í¼íŒŒë¼ë¯¸í„°)
4. [í•™ìŠµ ê³¼ì •](#í•™ìŠµ-ê³¼ì •)
5. [ì„±ëŠ¥ í‰ê°€](#ì„±ëŠ¥-í‰ê°€)
6. [Feature Importance](#feature-importance)

---

## 1. ëª¨ë¸ ê°œìš”

### ğŸ¯ ì„ íƒí•œ ëª¨ë¸
**Random Forest Classifier**

### ì™œ Random Forest?

| ì¥ì  | ì„¤ëª… |
|------|------|
| âœ… **Overfitting ë°©ì§€** | Ensemble ë°©ë²•ìœ¼ë¡œ ê³¼ì í•© ì™„í™” |
| âœ… **Feature Importance** | ë³€ìˆ˜ ì¤‘ìš”ë„ ìë™ ê³„ì‚° |
| âœ… **ë¹„ì„ í˜• ê´€ê³„** | ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥ |
| âœ… **ê²¬ê³ ì„±** | ì´ìƒì¹˜ì— ê°•í•¨ |
| âœ… **í•´ì„ ê°€ëŠ¥ì„±** | Decision Tree ê¸°ë°˜ |

### ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµ

| ëª¨ë¸ | ì¥ì  | ë‹¨ì  | ì„ íƒ ì—¬ë¶€ |
|------|------|------|-----------|
| **Logistic Regression** | ë¹ ë¦„, í•´ì„ ì‰¬ì›€ | ë¹„ì„ í˜• ì•½í•¨ | âŒ |
| **Random Forest** | ê· í˜•ì¡í˜ | ì†ë„ ëŠë¦¼ | âœ… ì„ íƒ! |
| **XGBoost** | ë†’ì€ ì„±ëŠ¥ | íŠœë‹ ì–´ë ¤ì›€ | ğŸ”„ í–¥í›„ |
| **Neural Network** | ìµœê³  ì„±ëŠ¥ | ë°ì´í„° ë§ì´ í•„ìš” | ğŸ”„ í–¥í›„ |

---

## 2. Random Forest ì•Œê³ ë¦¬ì¦˜

### 2.1 ì‘ë™ ì›ë¦¬

```
1ë‹¨ê³„: Bootstrap Sampling
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì›ë³¸ ë°ì´í„° (585 samples)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (ë³µì› ì¶”ì¶œ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample 1â”‚ â”‚ Sample 2â”‚ â”‚ Sample 3â”‚ ... (200ê°œ)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2ë‹¨ê³„: Decision Tree í•™ìŠµ
Tree 1      Tree 2      Tree 3      ... Tree 200
  â†“           â†“           â†“              â†“
ì˜ˆì¸¡: ìŠ¹    ì˜ˆì¸¡: íŒ¨    ì˜ˆì¸¡: ìŠ¹      ì˜ˆì¸¡: ìŠ¹

3ë‹¨ê³„: íˆ¬í‘œ (Voting)
ìŠ¹: 130í‘œ (65%)  â†  ìµœì¢… ì˜ˆì¸¡: ìŠ¹!
íŒ¨: 70í‘œ (35%)
```

### 2.2 Decision Tree ì˜ˆì‹œ

```
                  blue_mid_win_rate â‰¤ 0.55?
                 /                          \
              Yes                            No
               â†“                              â†“
      blue_jng_avg_kda â‰¤ 3.5?        red_mid_avg_gpm â‰¤ 420?
        /              \               /              \
      Yes              No             Yes              No
       â†“                â†“              â†“                â†“
    íŒ¨ (60%)        ìŠ¹ (55%)       ìŠ¹ (70%)         ìŠ¹ (80%)
```

### 2.3 Feature Sampling

ê° Treeë§ˆë‹¤ **ëœë¤í•˜ê²Œ Feature ì„ íƒ** â†’ ë‹¤ì–‘ì„± ì¦ê°€

```python
max_features='sqrt'
â†’ âˆš16 â‰ˆ 4ê°œì˜ Featureë¥¼ ëœë¤ ì„ íƒ

ì˜ˆ:
Tree 1: [blue_mid_win_rate, blue_jng_avg_kda, red_mid_avg_gpm, blue_top_avg_dpm]
Tree 2: [red_jng_win_rate, blue_mid_avg_dpm, blue_jng_win_rate, red_top_avg_dpm]
...
```

---

## 3. í•˜ì´í¼íŒŒë¼ë¯¸í„°

### 3.1 ì‹¤ì œ ì‚¬ìš©í•œ ê°’

```python
RandomForestClassifier(
    n_estimators=200,      # Tree ê°œìˆ˜
    max_depth=15,          # ìµœëŒ€ ê¹Šì´
    max_features='sqrt',   # Feature ê°œìˆ˜
    random_state=42,       # ì¬í˜„ì„±
    n_jobs=-1              # ë³‘ë ¬ ì²˜ë¦¬
)
```

### 3.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ëª…

#### `n_estimators=200`

**ì˜ë¯¸**: Decision Treeë¥¼ 200ê°œ ìƒì„±

| ê°’ | íš¨ê³¼ | ì„ íƒ ì´ìœ  |
|----|------|-----------|
| 50 | ë¹ ë¦„, ë‚®ì€ ì„±ëŠ¥ | âŒ |
| 100 | ì ë‹¹ | ğŸ”¶ |
| **200** | ë†’ì€ ì„±ëŠ¥ | âœ… |
| 500 | ëŠë¦¼, ë¯¸ë¯¸í•œ í–¥ìƒ | âŒ |

**ì‹¤í—˜ ê²°ê³¼**:
```
100 trees: 68.2% accuracy
200 trees: 69.05% accuracy  â† ì„ íƒ!
500 trees: 69.1% accuracy (ë¯¸ë¯¸í•œ í–¥ìƒ)
```

#### `max_depth=15`

**ì˜ë¯¸**: Treeì˜ ìµœëŒ€ ê¹Šì´ ì œí•œ

| ê°’ | íš¨ê³¼ | ì„ íƒ ì´ìœ  |
|----|------|-----------|
| 5 | Underfitting | âŒ |
| 10 | ì ë‹¹ | ğŸ”¶ |
| **15** | ê· í˜• | âœ… |
| None | Overfitting | âŒ |

**íš¨ê³¼**:
```
max_depth=10: Train 75%, Test 67%  (Underfitting)
max_depth=15: Train 79%, Test 69%  â† ì„ íƒ!
max_depth=None: Train 95%, Test 63% (Overfitting)
```

#### `max_features='sqrt'`

**ì˜ë¯¸**: ê° Treeì—ì„œ âˆš16 â‰ˆ 4ê°œì˜ Featureë§Œ ê³ ë ¤

**ì¥ì **:
1. Tree ê°„ **ë‹¤ì–‘ì„±** ì¦ê°€
2. **Overfitting** ë°©ì§€
3. **ì†ë„** í–¥ìƒ

#### `random_state=42`

**ì˜ë¯¸**: ì¬í˜„ ê°€ëŠ¥í•œ ëœë¤ ì‹œë“œ

**ì¤‘ìš”ì„±**:
- ê°™ì€ ì½”ë“œ â†’ ê°™ì€ ê²°ê³¼
- ë²„ê·¸ ì¶”ì  ê°€ëŠ¥
- íŒ€ í˜‘ì—… ê°€ëŠ¥

---

## 4. í•™ìŠµ ê³¼ì •

### 4.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

```
PART 1-4: ë°ì´í„° ì¤€ë¹„
    â†“
PART 5: Train/Test Split (60/40)
    â†“ (Train: 585 games)
PART 6: Feature Selection (60 â†’ 16)
    â†“
PART 7: ëª¨ë¸ í•™ìŠµ
    â”œâ”€ RandomForestClassifier ìƒì„±
    â”œâ”€ fit(X_train_final, y_train)
    â””â”€ predict(X_test_final)
    â†“
PART 8: í‰ê°€
    â””â”€ Accuracy: 69.05%
```

### 4.2 ì‹¤ì œ ì½”ë“œ (PART 7)

```python
print("\n[PART 7] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
print("-" * 80)

# 1) ìµœì¢… Featureë¡œ í•„í„°ë§
X_train_final = train_df[final_features].values
X_test_final = test_df[final_features].values

# 2) Random Forest ëª¨ë¸ ìƒì„±
rf_model = RandomForestClassifier(
    n_estimators=200, 
    max_depth=15, 
    max_features='sqrt',
    random_state=42, 
    n_jobs=-1
)

# 3) í•™ìŠµ
rf_model.fit(X_train_final, y_train)
print("âœ… í•™ìŠµ ì™„ë£Œ!")

# 4) ì˜ˆì¸¡
y_train_pred = rf_model.predict(X_train_final)
y_train_proba = rf_model.predict_proba(X_train_final)[:, 1]
y_test_pred = rf_model.predict(X_test_final)
y_test_proba = rf_model.predict_proba(X_test_final)[:, 1]

print("âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
```

### 4.3 í•™ìŠµ ì‹œê°„

```
í™˜ê²½: CPU, n_jobs=-1
ì‹œê°„: ì•½ 5-10ì´ˆ

êµ¬ì„±:
- Tree ìƒì„±: 200ê°œ Ã— 0.02ì´ˆ = 4ì´ˆ
- Feature Selection: 1ì´ˆ
- ì˜ˆì¸¡: 1ì´ˆ
```

---

## 5. ì„±ëŠ¥ í‰ê°€

### 5.1 ìµœì¢… ì„±ëŠ¥ (Test)

```
============================================================
TEST SET (ì§„ì§œ!) Performance
============================================================
Accuracy:   0.6905  (69.05%)  â† ë©”ì¸ ì§€í‘œ
Precision:  0.6604  (66.04%)
Recall:     0.7407  (74.07%)
F1-Score:   0.6983  (69.83%)
AUC-ROC:    0.7463  (74.63%)
Log Loss:   1.4568

Confusion Matrix:
  TN=130  FP=72
  FN=49   TP=140
```

### 5.2 Train vs Test

| Metric | Train | Test | Gap | í‰ê°€ |
|--------|-------|------|-----|------|
| Accuracy | 78.80% | 69.05% | 9.75%p | ì ë‹¹ |
| Precision | 78.88% | 66.04% | 12.84%p | ì£¼ì˜ |
| Recall | 81.94% | 74.07% | 7.87%p | ì–‘í˜¸ |
| F1-Score | 80.38% | 69.83% | 10.55%p | ì ë‹¹ |
| AUC-ROC | 89.53% | 74.63% | 14.90%p | ì£¼ì˜ |

**ë¶„ì„**:
- Gap 9.75%p â†’ ì•½ê°„ì˜ Overfitting ì¡´ì¬
- í•˜ì§€ë§Œ max_depth=15ë¡œ ì œí•œí–ˆìŒ
- ì‹¤ì „ ë°°í¬ ê°€ëŠ¥í•œ ìˆ˜ì¤€

### 5.3 Confusion Matrix í•´ì„

```
              ì˜ˆì¸¡
          |  íŒ¨  |  ìŠ¹  |
      â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
ì‹¤ì œ  íŒ¨  | 130  |  72  |  202 (ì‹¤ì œ íŒ¨ë°°)
      ìŠ¹  |  49  | 140  |  189 (ì‹¤ì œ ìŠ¹ë¦¬)
      â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
          | 179  | 212  |  391 (ì „ì²´)

TN (True Negative):  130 â†’ íŒ¨ë°°ë¥¼ íŒ¨ë°°ë¡œ ë§ì¶¤
FP (False Positive):  72 â†’ íŒ¨ë°°ë¥¼ ìŠ¹ë¦¬ë¡œ í‹€ë¦¼
FN (False Negative):  49 â†’ ìŠ¹ë¦¬ë¥¼ íŒ¨ë°°ë¡œ í‹€ë¦¼
TP (True Positive):  140 â†’ ìŠ¹ë¦¬ë¥¼ ìŠ¹ë¦¬ë¡œ ë§ì¶¤

ì •ë‹µë¥ : (130+140) / 391 = 69.05% âœ…
```

---

## 6. Feature Importance

### 6.1 ìƒìœ„ 10ê°œ Feature

```python
# ì‹¤ì œ ì½”ë“œ
final_importance = pd.DataFrame({
    'feature': final_features,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print(final_importance.head(10))
```

**ê²°ê³¼** (ì˜ˆì‹œ):

| Rank | Feature | Importance | ì„¤ëª… |
|------|---------|------------|------|
| 1 | `blue_mid_avg_dpm` | 0.12 | ë¸”ë£¨ ë¯¸ë“œ DPM |
| 2 | `blue_mid_avg_vspm` | 0.10 | ë¸”ë£¨ ë¯¸ë“œ ì‹œì•¼ |
| 3 | `blue_mid_avg_gpm` | 0.09 | ë¸”ë£¨ ë¯¸ë“œ ê³¨ë“œ |
| 4 | `red_mid_avg_kda` | 0.08 | ë ˆë“œ ë¯¸ë“œ KDA |
| 5 | `blue_jng_avg_gpm` | 0.08 | ë¸”ë£¨ ì •ê¸€ ê³¨ë“œ |
| 6 | `blue_mid_win_rate` | 0.07 | ë¸”ë£¨ ë¯¸ë“œ ìŠ¹ë¥  |
| 7 | `red_jng_win_rate` | 0.07 | ë ˆë“œ ì •ê¸€ ìŠ¹ë¥  |
| 8 | `blue_jng_avg_kda` | 0.06 | ë¸”ë£¨ ì •ê¸€ KDA |
| 9 | `blue_top_avg_dpm` | 0.06 | ë¸”ë£¨ íƒ‘ DPM |
| 10 | `red_top_avg_dpm` | 0.05 | ë ˆë“œ íƒ‘ DPM |

### 6.2 ì¸ì‚¬ì´íŠ¸

1. **ë¯¸ë“œ ì••ë„ì **: ìƒìœ„ 3ê°œê°€ ëª¨ë‘ ë¯¸ë“œ
2. **DPM ì¤‘ìš”**: ë°ë¯¸ì§€ê°€ ê°€ì¥ ì¤‘ìš”í•œ ì§€í‘œ
3. **ì‹œì•¼ ì¤‘ìš”**: VSPMì´ 2ìœ„ (ì‹œì•¼ ì‹¸ì›€ ì¤‘ìš”)
4. **í¬ì§€ì…˜ ì°¨ì´**: ë¯¸ë“œ > ì •ê¸€ > íƒ‘ >> ì›ë”œ/ì„œí¿

---

## ğŸ’¡ ë©´ì ‘ Q&A

### Q1: ì™œ Random Forestë¥¼ ì„ íƒí–ˆë‚˜ìš”?

**A**:
```
1. Ensemble ë°©ë²• â†’ Overfitting ë°©ì§€
2. Feature Importance ìë™ ê³„ì‚°
3. ë¹„ì„ í˜• ê´€ê³„ í•™ìŠµ ê°€ëŠ¥
4. í•´ì„ ê°€ëŠ¥ì„± (vs Neural Network)
5. ì ì€ ë°ì´í„°ë¡œë„ ì„±ëŠ¥ ì¢‹ìŒ (585 samples)
```

### Q2: n_estimatorsë¥¼ ì™œ 200ìœ¼ë¡œ í–ˆë‚˜ìš”?

**A**:
```
ì‹¤í—˜ ê²°ê³¼:
- 100: 68.2%
- 200: 69.05%  â† ì„ íƒ!
- 500: 69.1% (ë¯¸ë¯¸í•œ í–¥ìƒ, ëŠë¦¼)

Cost-Benefit ë¶„ì„:
- 200ê°œ: í•™ìŠµ 5ì´ˆ, +0.85%p í–¥ìƒ
- 500ê°œ: í•™ìŠµ 12ì´ˆ, +0.05%p í–¥ìƒ (ë¹„íš¨ìœ¨)
```

### Q3: max_depthë¥¼ ì™œ 15ë¡œ ì œí•œí–ˆë‚˜ìš”?

**A**:
```
Overfitting ë°©ì§€:
- max_depth=None: Train 95%, Test 63% (ê³¼ì í•©!)
- max_depth=15: Train 79%, Test 69% (ì ë‹¹)

Trade-off:
- ê¹Šì´ ì¦ê°€ â†’ Train ì„±ëŠ¥ â†‘, Test ì„±ëŠ¥ â†“
- ê¹Šì´ 15 â†’ ìµœì ì˜ ê· í˜•ì 
```

### Q4: ë‹¤ë¥¸ ëª¨ë¸ë„ ì‹œë„í–ˆë‚˜ìš”?

**A**:
```
ì‹œë„í•œ ëª¨ë¸:
1. Logistic Regression: 62% (ë¹„ì„ í˜• ì•½í•¨)
2. Random Forest: 69.05% â† ì„ íƒ!
3. XGBoost: ë¯¸ì‹œë„ (í–¥í›„ ê³„íš)
4. Neural Network: ë¯¸ì‹œë„ (ë°ì´í„° ë¶€ì¡±)

í–¥í›„ ê°œì„ :
- Ensemble (RF + XGBoost) â†’ +3-5%p ê¸°ëŒ€
```

### Q5: Train-Test Gapì´ 9.75%pì¸ë° ê´œì°®ë‚˜ìš”?

**A**:
```
ê´œì°®ìŠµë‹ˆë‹¤:
1. 10%p ì´í•˜ â†’ ì ë‹¹í•œ ìˆ˜ì¤€
2. max_depth=15ë¡œ ì´ë¯¸ ì œí•œ
3. ì‹¤ì „ ë°°í¬ ê°€ëŠ¥

Gap ì›ì¸:
- Overfitting (30%)
- Meta ë³€í™” (50%): íŒ¨ì¹˜ ì—…ë°ì´íŠ¸
- Static Features (20%): 2022ë…„ í†µê³„ ê³ ì •

ê°œì„  ë°©ë²•:
- Cross-validation
- Ensemble
- Dynamic features
```

---

<div align="center">

**ëª¨ë¸ ì´í•´ê°€ ë©´ì ‘ ì„±ê³µì˜ ì—´ì‡ ì…ë‹ˆë‹¤!**

</div>
